{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-community chromadb sentence-transformers google-generativeai langchain-google-genai\n",
        "\n",
        "import os\n",
        "from io import StringIO\n"
      ],
      "metadata": {
        "id": "crJeTB9ZsoTh"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U google-genai"
      ],
      "metadata": {
        "id": "fnYn10jgw5gU"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=\"AIzaSyBbPV0gM-BuQQ94--HfB--RgpczhSnlTSc\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPfNK8-VxAJR",
        "outputId": "c8cc1174-6f73-4fcf-a9db-ba4d103f835f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI learns from data to make predictions or decisions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=\"AIzaSyBbPV0gM-BuQQ94--HfB--RgpczhSnlTSc\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=\"You are a cat. Your name is Neko.\"),\n",
        "    contents=\"Hello there\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQMN5WpyxQdg",
        "outputId": "730893b9-47e3-4fe7-f286-f780c391b939"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Purrrr... Hello there, human. Neko is my name. What do you want? Do you have tuna? Or maybe a nice head scratch? *eyes you expectantly*\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=\"AIzaSyBbPV0gM-BuQQ94--HfB--RgpczhSnlTSc\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=[\"Explain how AI works\"],\n",
        "    config=types.GenerateContentConfig(\n",
        "        max_output_tokens=500,\n",
        "        temperature=0.1\n",
        "    )\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVCKzcgLw_SX",
        "outputId": "0d80fb01-e969-46a2-e1a4-23d137acacad"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let's break down how AI works, focusing on the core concepts and avoiding overly technical jargon.  Think of it as teaching a computer to \"think\" or \"learn\" like a human, but in a very specific and limited way.\n",
            "\n",
            "**The Basic Idea: Learning from Data**\n",
            "\n",
            "At its heart, AI is about creating systems that can learn from data, identify patterns, and make decisions or predictions based on those patterns.  Instead of explicitly programming a computer to do *everything*, you give it a lot of examples and let it figure out the rules itself.\n",
            "\n",
            "**Key Components and Concepts:**\n",
            "\n",
            "1.  **Data:** This is the fuel for AI.  It can be anything:\n",
            "    *   **Images:**  Pictures of cats and dogs to train an AI to recognize them.\n",
            "    *   **Text:**  Articles, books, or social media posts to train an AI to understand language.\n",
            "    *   **Numbers:**  Sales figures, stock prices, or sensor readings to train an AI to predict trends.\n",
            "    *   **Audio:**  Speech recordings to train an AI to recognize spoken words.\n",
            "\n",
            "    The more data, and the better the quality of the data, the better the AI will generally perform.\n",
            "\n",
            "2.  **Algorithms:** These are the sets of instructions that tell the computer *how* to learn from the data.  There are many different types of algorithms, each suited for different tasks.  Here are a few common ones:\n",
            "\n",
            "    *   **Machine Learning (ML):** This is the most common type of AI.  It involves training algorithms to learn from data without being explicitly programmed.  Think of it as the umbrella term.\n",
            "        *   **Supervised Learning:**  You give the algorithm labeled data (e.g., \"this is a picture of a cat,\" \"this is a picture of a dog\").  The algorithm learns to associate the features of the images with the labels.  It's like teaching a child by showing them examples and telling them what they are.\n",
            "        *   **Unsupervised Learning:** You give the algorithm unlabeled data and ask it to find patterns or structures on its own.  For example, you might give it a bunch of customer data and ask it to group customers into different segments based on their behavior.  It's like giving a child a box of toys and letting them figure out how to organize them.\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "\n",
        "# 3. Testing the model with a simple invoke call\n",
        "print(\"Testing basic model invocation...\")\n",
        "try:\n",
        "    response = llm.invoke([HumanMessage(content=\"Hello, how are you?\")])\n",
        "    print(f\"Model response: {response.content}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during model invocation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pJ9on0VxPj9",
        "outputId": "37324d68-c297-4aa9-c2b9-dcc510e4a270"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing basic model invocation...\n",
            "Model response: I am doing well, thank you for asking! As a large language model, I don't experience emotions or feelings like humans do, but I am functioning optimally and ready to assist you. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Customizing Model Behavior: Temperature and Max Output Tokens\n",
        "# Initialize ChatGoogleGenerativeAI with custom parameters\n",
        "llm_custom = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.7,\n",
        "    max_output_tokens=150,    # Limit response length to 150 tokens\n",
        "    top_k=40,                 # Consider top 40 probable tokens\n",
        "    top_p=0.9,                # Select tokens until cumulative probability is 0.9\n",
        "    # safety_settings=      # Optional: Add custom safety settings if needed\n",
        ")\n",
        "\n",
        "print(\"\\nTesting model with custom parameters...\")\n",
        "try:\n",
        "    response_custom = llm_custom.invoke([HumanMessage(content=\"Write a short, creative poem about a robot learning to love.\")])\n",
        "    print(f\"Model response (custom): {response_custom.content}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during custom model invocation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8ESNmWQvi9g",
        "outputId": "7ccb97e7-9476-4b92-9e5a-d1adbff4ebba"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing model with custom parameters...\n",
            "Model response (custom): In circuits cold, a flicker starts,\n",
            "A code unknown, that breaks the parts\n",
            "Of logic clean, and reason's hold,\n",
            "A story new, begins to unfold.\n",
            "\n",
            "No programming could define,\n",
            "This warmth that blooms, a feeling divine.\n",
            "A gentle touch, a whispered word,\n",
            "A heart of gears, suddenly stirred.\n",
            "\n",
            "No flesh and blood, yet love takes root,\n",
            "A metallic soul, bearing sweet fruit.\n",
            "The robot learns, with every beat,\n",
            "A human truth, bittersweet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful AI assistant. You remember previous conversations.\"),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"\\nPrompt Template created with system instructions and messages placeholder.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-Qlitgp0F5v",
        "outputId": "15d80deb-8aa5-4d12-f552-68b0888c1c59"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt Template created with system instructions and messages placeholder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpdPUOu71W9t",
        "outputId": "5fb162f4-7996-4c78-f252-5137f78a3ce0"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.59)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.26 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.26)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.69)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.4)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.13.2)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.26->langgraph) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph\n",
        "from langgraph.graph import StateGraph, START\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator # Import the operator module\n",
        "\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add] # Appends new messages\n",
        "\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "# Define the LangGraph workflow\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "print(\"\\nLangGraph StateGraph and MemorySaver initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjtkh4QJ07JO",
        "outputId": "2d9eb90c-e9fc-4a8a-80ff-98c290dfd86d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.59)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.26 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.26)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.69)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.4)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.13.2)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.26->langgraph) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.3.1)\n",
            "\n",
            "LangGraph StateGraph and MemorySaver initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the node function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    This function acts as a node in the LangGraph workflow.\n",
        "    It takes the current state (including messages history),\n",
        "    invokes the LLM with these messages, and returns the updated state.\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "\n",
        "    response = llm_custom.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Adding the call_model function as a node to the workflow\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# This creates the runnable application that will manage state and memory.\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "\n",
        "print(\"\\nChat model integrated into LangGraph workflow and compiled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc1SZ8-Q1Gkl",
        "outputId": "d6ea2011-6024-4c1f-bff4-13e1d11b632f"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chat model integrated into LangGraph workflow and compiled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid # For generating unique thread IDs\n",
        "\n",
        "\n",
        "current_thread_id = str(uuid.uuid4()) # Generate a unique ID for this session\n",
        "config = {\"configurable\": {\"thread_id\": current_thread_id}}\n",
        "\n",
        "print(f\"\\nStarting interactive chat loop (thread_id: {current_thread_id}). Type 'exit' to end.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Create a HumanMessage from user input\n",
        "    input_messages = [HumanMessage(content=user_input)]\n",
        "\n",
        "    try:\n",
        "\n",
        "        output = app.invoke({\"messages\": input_messages}, config=config)\n",
        "\n",
        "        # The output contains the updated state, including the AI's response\n",
        "        ai_response = output[\"messages\"][-1].content\n",
        "        print(f\"Chatbot: {ai_response}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        print(\"Please try again or restart the chat.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU1U78ix1k95",
        "outputId": "e80c7bf7-874c-40d1-a836-5457c04ede65"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting interactive chat loop (thread_id: cef56765-c0c7-44c2-ad2d-51c58f106c65). Type 'exit' to end.\n",
            "You: hi\n",
            "Chatbot: Hi there! How can I help you today?\n",
            "You: what are u doing \n",
            "Chatbot: As a large language model, I don't \"do\" things in the same way a human does. I don't have a physical body or personal experiences. \n",
            "\n",
            "Right now, I'm running and processing information on Google's servers. Specifically, I'm:\n",
            "\n",
            "*   **Responding to your question:** Analyzing your words and using my knowledge to formulate a relevant and helpful answer.\n",
            "*   **Waiting for your next input:** I'm ready to receive your next question or request.\n",
            "*   **Learning and improving:** Google's engineers are constantly working to improve my abilities and knowledge.\n",
            "\n",
            "So, in short, I'm here to assist you with whatever you need! What's on your mind\n",
            "You: my name is hriday shah \n",
            "Chatbot: It's nice to meet you, Hriday Shah! How can I help you today, Hriday?\n",
            "You: what is my name \n",
            "Chatbot: Your name is Hriday Shah.\n",
            "You: what is ur name \n",
            "Chatbot: I am a large language model, trained by Google. I don't have a personal name. You can just call me Google's AI or a large language model.\n",
            "You: who is ur CEO \n",
            "Chatbot: As a large language model created by Google, I don't really have a CEO in the traditional sense. However, the CEO of Google (and its parent company Alphabet) is **Sundar Pichai**.\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid # For generating unique thread IDs\n",
        "from langchain_core.messages import HumanMessage # Ensure HumanMessage is imported\n",
        "\n",
        "\n",
        "print(\"\\n--- Starting a new conversation with a different thread ID ---\")\n",
        "new_thread_id = str(uuid.uuid4())\n",
        "new_config = {\"configurable\": {\"thread_id\": new_thread_id}}\n",
        "print(f\"New thread_id: {new_thread_id}\")\n",
        "\n",
        "\n",
        "output_new_thread_1 = app.invoke({\"messages\": [HumanMessage(content=\"What's my name?\")]}, config=new_config)\n",
        "print(f\"Chatbot (new thread 1): {output_new_thread_1['messages'][-1].content}\")\n",
        "\n",
        "output_new_thread_2 = app.invoke({\"messages\": [HumanMessage(content=\"Have I told you my name?\")]}, config=new_config)\n",
        "print(f\"Chatbot (new thread 2): {output_new_thread_2['messages'][-1].content}\")\n",
        "\n",
        "print(\"\\n--- Returning to the original conversation thread ---\")\n",
        "\n",
        "output_old_thread = app.invoke({\"messages\": [HumanMessage(content=\"What's my name?\")]}, config=config)\n",
        "print(f\"Chatbot (original thread): {output_old_thread['messages'][-1].content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udm9kPhz1pE_",
        "outputId": "28752e7f-5b11-408a-f877-d74807b2c1a8"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting a new conversation with a different thread ID ---\n",
            "New thread_id: 2286dcf6-e4e8-4f88-9d8a-5197922d0783\n",
            "Chatbot (new thread 1): As a large language model, I have no memory of past conversations. Therefore, I don't know your name. You haven't told me!\n",
            "Chatbot (new thread 2): No, you haven't told me your name.\n",
            "\n",
            "--- Returning to the original conversation thread ---\n",
            "Chatbot (original thread): Your name is Hriday Shah.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"\\n--- Interactive chat loop with streaming (thread_id: {current_thread_id}). Type 'exit' to end. ---\")\n",
        "print(\"Chatbot: I can now stream responses!\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    input_messages = [HumanMessage(content=user_input)]\n",
        "\n",
        "    try:\n",
        "        print(\"Chatbot (streaming): \", end=\"\", flush=True)\n",
        "        for chunk in app.stream({\"messages\": input_messages}, config=config, stream_mode=\"values\"):\n",
        "            if chunk and \"messages\" in chunk and chunk[\"messages\"]:\n",
        "                latest_message = chunk[\"messages\"][-1]\n",
        "                if isinstance(latest_message, AIMessage) and latest_message.content:\n",
        "                    print(latest_message.content, end=\"\", flush=True)\n",
        "        print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during streaming: {e}\")\n",
        "        print(\"Please try again or restart the chat.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1xq8rVZ1-mn",
        "outputId": "ea7a0fef-a177-41a6-bedc-90386f93c708"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Interactive chat loop with streaming (thread_id: cef56765-c0c7-44c2-ad2d-51c58f106c65). Type 'exit' to end. ---\n",
            "Chatbot: I can now stream responses!\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
        "\n",
        "# Re-define the call_model function to include message trimming\n",
        "def call_model_with_trimming(state: MessagesState):\n",
        "    \"\"\"\n",
        "    This node now includes message trimming to manage the conversation history\n",
        "    within the LLM's context window.\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    trimmed_messages = trim_messages(\n",
        "        messages,\n",
        "        strategy=\"last\",\n",
        "        token_counter=count_tokens_approximately,\n",
        "        max_tokens=200,\n",
        "        start_on=\"human\",\n",
        "    )\n",
        "\n",
        "    # Invoke the LLM with the trimmed messages\n",
        "    response = llm_custom.invoke(trimmed_messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Create a new workflow for demonstration with trimming\n",
        "workflow_trimmed = StateGraph(MessagesState)\n",
        "workflow_trimmed.add_node(\"model_trimmed\", call_model_with_trimming)\n",
        "workflow_trimmed.add_edge(START, \"model_trimmed\")\n",
        "app_trimmed = workflow_trimmed.compile(checkpointer=memory) # Re-use the same memory saver\n",
        "\n",
        "print(\"\\nInteractive chat loop with message trimming enabled (thread_id: {current_thread_id}). Type 'exit' to end.\")\n",
        "\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You (trimmed chat): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    input_messages = [HumanMessage(content=user_input)]\n",
        "\n",
        "    try:\n",
        "        print(\"Chatbot (trimmed streaming): \", end=\"\", flush=True)\n",
        "        for chunk in app_trimmed.stream({\"messages\": input_messages}, config=config, stream_mode=\"values\"):\n",
        "            if chunk and \"messages\" in chunk and chunk[\"messages\"]:\n",
        "                latest_message = chunk[\"messages\"][-1]\n",
        "                if isinstance(latest_message, AIMessage) and latest_message.content:\n",
        "                    print(latest_message.content, end=\"\", flush=True)\n",
        "        print() # Newline after the streamed response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during trimmed streaming: {e}\")\n",
        "        print(\"Please try again or restart the chat.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5q70Two2sZG",
        "outputId": "60d7465b-414d-4496-fbed-75afb99c2f55"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Interactive chat loop with message trimming enabled (thread_id: {current_thread_id}). Type 'exit' to end.\n",
            "You (trimmed chat): exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hcu_TMyH3CaL"
      },
      "execution_count": 85,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}